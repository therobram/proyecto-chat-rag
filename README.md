# ğŸ¤– Proyecto Chat RAG - Plataforma Inteligente de Consulta de Documentos

Sistema de chat inteligente con Retrieval Augmented Generation (RAG) que permite consultar cualquier tipo de documentos de forma conversacional, construido con FastAPI, LangChain y Ollama.

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **Chat inteligente en espaÃ±ol** con modelos locales de Ollama
- ğŸ“„ **Procesamiento avanzado de documentos** (PDF, DOCX, TXT, MD, PPTX, HTML)
- ğŸ–¼ï¸ **ExtracciÃ³n de texto de imÃ¡genes** con OCR (Tesseract)
- ğŸµ **TranscripciÃ³n de audio** con Whisper AI
- ğŸ§  **RAG profesional** con memoria conversacional
- ğŸ” **BÃºsqueda semÃ¡ntica** en base de datos vectorial
- ğŸ¯ **Modelos configurables** desde ultra-ligeros hasta alta calidad
- ğŸ³ **Containerizado** con Docker Compose
- ğŸŒ **Interfaz web** moderna y responsiva

## ğŸš€ Inicio RÃ¡pido

### Prerequisitos
- Docker y Docker Compose
- Git

### InstalaciÃ³n

```bash
# Clonar el repositorio
git clone <tu-repositorio>
cd proyecto-chat-rag

# Configurar variables de entorno
cp .env.example .env

# Iniciar el sistema
docker-compose up --build
```

### Acceso
- **AplicaciÃ³n web**: http://localhost:8000
- **API Ollama**: http://localhost:11434

## âš™ï¸ ConfiguraciÃ³n

### Modelos Disponibles

| Modelo | TamaÃ±o | RAM | Velocidad | Uso Recomendado |
|--------|--------|-----|-----------|-----------------|
| `tinyllama` | 1.1B | 1-2GB | âš¡âš¡âš¡ | Desarrollo/Testing |
| `gemma2:2b` | 2B | 2-3GB | âš¡âš¡ | ProducciÃ³n ligera |
| `llama3.2:3b` | 3B | 4-6GB | âš¡ | Equilibrado |
| `mistral:7b` | 7B | 8GB | ğŸŒ | Alta calidad |

### Cambiar Modelo

```bash
# OpciÃ³n 1: Editar .env
OLLAMA_MODEL=gemma2:2b

# OpciÃ³n 2: Variable de entorno
OLLAMA_MODEL=mistral:7b docker-compose up --build
```

### Variables de Entorno (.env)

```env
# Modelo de IA
OLLAMA_MODEL=tinyllama
OLLAMA_HOST=http://ollama:11434

# ConfiguraciÃ³n de la aplicaciÃ³n
APP_HOST=0.0.0.0
APP_PORT=8000
DATA_DIR=data
VECTOR_DIR=vectorstore

# ConfiguraciÃ³n RAG
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_K_DOCUMENTS=3
```

## ğŸ“Š Uso

### 1. Cargar Documentos
- Accede a http://localhost:8000
- Usa la secciÃ³n "Cargar Archivos" 
- **Documentos**: PDF, DOCX, TXT, MD, PPTX, HTML
- **ImÃ¡genes**: PNG, JPG, JPEG (extrae texto con OCR)
- **Audio**: MP3, WAV, M4A (transcribe con Whisper)
- Los archivos se procesan automÃ¡ticamente

### 2. Hacer Consultas
- Escribe tu pregunta en el chat
- El sistema responde basÃ¡ndose en los documentos cargados
- Incluye citas de las fuentes utilizadas
- Mantiene contexto conversacional

### 3. API Endpoints

```bash
# InformaciÃ³n de modelos
GET /api/models

# Chat
POST /chat
Content-Type: application/x-www-form-urlencoded
message=Â¿CuÃ¡l es el proceso de registro?

# Subir documentos
POST /upload
Content-Type: multipart/form-data

# Limpiar conversaciÃ³n
POST /chat/clear

# Estado de documentos
GET /api/documents
```

## ğŸ—ï¸ Arquitectura

```
proyecto-chat-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI + RAG logic
â”‚   â”œâ”€â”€ static/              # CSS, JS, assets
â”‚   â””â”€â”€ templates/           # HTML templates
â”œâ”€â”€ data/                    # Documentos (local)
â”œâ”€â”€ vectorstore/             # Base datos vectorial (local)
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n
â”œâ”€â”€ Dockerfile              # Imagen de la app
â”œâ”€â”€ requirements.txt        # Dependencias Python
â””â”€â”€ .env                    # ConfiguraciÃ³n
```

### Componentes

- **FastAPI**: API REST y servidor web
- **LangChain**: Framework RAG y procesamiento
- **Ollama**: Modelos de IA locales
- **ChromaDB**: Base de datos vectorial
- **Docker**: ContainerizaciÃ³n

## ğŸ”§ Desarrollo

### Ejecutar en modo desarrollo

```bash
# Con logs detallados
docker-compose up --build

# Solo la aplicaciÃ³n (Ollama en background)
docker-compose up app

# Reconstruir solo la app
docker-compose build app && docker-compose up app
```

### Logs y debugging

```bash
# Ver logs
docker-compose logs -f app
docker-compose logs -f ollama

# Acceder al contenedor
docker-compose exec app bash
```

## ğŸ› ï¸ SoluciÃ³n de Problemas

### Problemas Comunes

**Error: "Collection expecting embedding with dimension"**
```bash
# Limpiar vectorstore
docker-compose down
docker volume rm proyecto-chat-rag_ollama_models
docker-compose up --build
```

**Respuestas lentas**
- Usar modelo mÃ¡s ligero: `tinyllama` o `gemma2:2b`
- Verificar recursos disponibles
- Reducir `RAG_K_DOCUMENTS` en `.env`

**Error de archivos PDF**
- Verificar que el PDF no estÃ© corrupto
- Probar con formato TXT primero
- Revisar logs: `docker-compose logs app`

**Puerto ocupado**
```bash
# Cambiar puertos en docker-compose.yml
ports:
  - "8001:8000"  # App
  - "11435:11434"  # Ollama
```

## ğŸ“‹ Requisitos del Sistema

### MÃ­nimos
- RAM: 4GB
- CPU: 2 cores
- Disco: 2GB libre
- Docker: 20.x+

### Recomendado
- RAM: 8GB+
- CPU: 4+ cores
- Disco: 5GB libre
- SSD para mejor rendimiento

## ğŸ“ Casos de Uso

### ğŸ¥ **Sector MÃ©dico**
- Consultas sobre expedientes mÃ©dicos
- AnÃ¡lisis de estudios e imÃ¡genes mÃ©dicas
- TranscripciÃ³n de consultas grabadas

### ğŸ“š **EducaciÃ³n**
- Base de conocimiento acadÃ©mica
- Consultas sobre libros y papers
- AnÃ¡lisis de contenido educativo

### âš–ï¸ **Legal**
- BÃºsqueda en documentos legales
- AnÃ¡lisis de contratos y normativas
- Consultas sobre jurisprudencia

### ğŸ¢ **Empresarial**
- DocumentaciÃ³n tÃ©cnica
- Manuales y procedimientos
- AnÃ¡lisis de reportes empresariales

### ğŸ”¬ **InvestigaciÃ³n**
- Base de papers cientÃ­ficos
- AnÃ¡lisis de datos de investigaciÃ³n
- Consultas sobre bibliografÃ­a

### ğŸ’¼ **Cualquier sector**
- Adaptable a cualquier dominio
- Procesa documentos especÃ­ficos del negocio
- Personalizable segÃºn necesidades

## ğŸ”’ Privacidad y Seguridad

- âœ… **100% Local**: Todo funciona en tu infraestructura
- âœ… **Sin envÃ­o de datos**: No se conecta a servicios externos
- âœ… **Documentos seguros**: Permanecen en tu mÃ¡quina
- âœ… **Modelos offline**: IA ejecuta completamente local
- âœ… **Escalable**: Desde laptop personal hasta servidores empresariales

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

---

**Plataforma RAG universal - Adaptable a cualquier industria y caso de uso** ğŸš€

