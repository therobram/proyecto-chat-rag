# ğŸ¤– Chat RAG - Sistema de ConversaciÃ³n con Documentos

Un sistema RAG (Retrieval-Augmented Generation) moderno que permite conversar con tus documentos usando modelos de lenguaje local con Ollama.

## âœ¨ CaracterÃ­sticas

- ğŸš€ **Modelos configurables**: 7 opciones desde ultra-ligero (1GB RAM) hasta alta calidad (8GB RAM)
- ğŸ“„ **MÃºltiples formatos**: PDF, TXT, DOCX y mÃ¡s
- ğŸ’¬ **Chat inteligente**: Memoria conversacional y contexto persistente
- ğŸ³ **Docker**: ConfiguraciÃ³n completa con Docker Compose
- âš¡ **Optimizado**: Arquitectura ligera y eficiente
- ğŸ¨ **Interfaz moderna**: UI responsiva con informaciÃ³n del modelo

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos
- Docker y Docker Compose
- Al menos 2GB de RAM libre (segÃºn modelo)

### 1. Clonar y configurar
```bash
git clone <tu-repo>
cd proyecto-chat-rag
```

### 2. Configurar modelo (opcional)
```bash
# Editar .env para cambiar modelo
OLLAMA_MODEL=tinyllama  # Modelo por defecto (ultra-rÃ¡pido)
```

### 3. Ejecutar
```bash
docker-compose up --build
```

### 4. Usar
- **Web UI**: http://localhost:8000
- **API**: http://localhost:8000/docs

## ğŸ›ï¸ ConfiguraciÃ³n de Modelos

### Modelos Disponibles

| Modelo | TamaÃ±o | RAM | Velocidad | Calidad | Uso Recomendado |
|--------|--------|-----|-----------|---------|-----------------|
| `tinyllama` | 1.1B | 1-2GB | âš¡âš¡âš¡ | â­â­ | Desarrollo/Testing |
| `gemma2:2b` | 2B | 2-3GB | âš¡âš¡ | â­â­â­ | ProducciÃ³n ligera |
| `qwen2:1.5b` | 1.5B | 2-3GB | âš¡âš¡ | â­â­â­ | Equilibrio ligero |
| `llama3.2:3b` | 3B | 4-6GB | âš¡ | â­â­â­â­ | Equilibrado |
| `phi3:3.8b` | 3.8B | 4-6GB | âš¡ | â­â­â­â­ | Optimizado |
| `llama3.1:8b` | 8B | 8GB | ğŸŒ | â­â­â­â­â­ | Alta calidad |
| `mistral:7b` | 7B | 8GB | ğŸŒ | â­â­â­â­â­ | Mistral AI |

### Cambiar Modelo

#### OpciÃ³n 1: Script automÃ¡tico (Recomendado)
```bash
./change-model.sh tinyllama      # Ultra rÃ¡pido
./change-model.sh gemma2:2b      # Muy eficiente  
./change-model.sh llama3.2:3b    # Equilibrado
./change-model.sh mistral:7b     # Alta calidad
```

#### OpciÃ³n 2: Variable de entorno
```bash
OLLAMA_MODEL=gemma2:2b docker-compose up --build
```

#### OpciÃ³n 3: Editar .env
```env
OLLAMA_MODEL=tinyllama  # Cambiar por el modelo deseado
```

## ğŸ“ Estructura del Proyecto

```
proyecto-chat-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ models_config.py     # ConfiguraciÃ³n de modelos
â”‚   â”œâ”€â”€ static/              # Archivos estÃ¡ticos
â”‚   â”‚   â”œâ”€â”€ css/style.css    # Estilos
â”‚   â”‚   â””â”€â”€ js/chat.js       # JavaScript del chat
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ chat.html        # Interfaz web
â”œâ”€â”€ data/                    # Documentos cargados
â”œâ”€â”€ vectorstore/            # Base de datos vectorial
â”œâ”€â”€ .env                    # Variables de entorno
â”œâ”€â”€ docker-compose.yml      # ConfiguraciÃ³n Docker
â”œâ”€â”€ Dockerfile             # Imagen de la aplicaciÃ³n
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ change-model.sh        # Script cambio de modelo
â””â”€â”€ README.md             # Esta documentaciÃ³n
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno (.env)

```env
# Modelo de IA
OLLAMA_MODEL=tinyllama
OLLAMA_HOST=http://ollama:11434

# AplicaciÃ³n
APP_HOST=0.0.0.0
APP_PORT=8000
DATA_DIR=data
VECTOR_DIR=vectorstore

# RAG
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_K_DOCUMENTS=3
```

### Personalizar Docker Compose

```yaml
services:
  app:
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-tinyllama}
    volumes:
      - ./custom-data:/app/data  # Directorio personalizado
  
  ollama:
    environment:
      - OLLAMA_KEEP_ALIVE=24h    # Mantener modelo en memoria
```

## ğŸ“¡ API Endpoints

### Principales
- `GET /` - Interfaz web
- `POST /upload` - Cargar documentos
- `POST /chat` - Enviar mensaje al chat

### InformaciÃ³n
- `GET /api/models` - Listar modelos disponibles
- `GET /api/model/current` - InformaciÃ³n del modelo actual
- `GET /docs` - DocumentaciÃ³n Swagger

### Ejemplos de API

```bash
# InformaciÃ³n del modelo actual
curl http://localhost:8000/api/model/current

# Cargar documento
curl -X POST -F "files=@documento.pdf" http://localhost:8000/upload

# Enviar mensaje
curl -X POST -F "message=Â¿QuÃ© dice el documento?" http://localhost:8000/chat
```

## ğŸ’¡ Uso

### 1. Cargar Documentos
- Usa la interfaz web o API
- Formatos soportados: PDF, TXT, DOCX
- Los documentos se procesan automÃ¡ticamente

### 2. Hacer Preguntas
- El chat mantiene contexto de la conversaciÃ³n
- Las respuestas incluyen informaciÃ³n de los documentos
- Soporta preguntas de seguimiento

### 3. Gestionar Modelos
- Cambiar modelo sin perder datos
- InformaciÃ³n en tiempo real del modelo usado
- OptimizaciÃ³n automÃ¡tica segÃºn recursos

## ğŸ› ï¸ Desarrollo

### Estructura del CÃ³digo

```python
# app/main.py - AplicaciÃ³n principal
from fastapi import FastAPI
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# ConfiguraciÃ³n dinÃ¡mica de modelos
MODEL_NAME = os.getenv("OLLAMA_MODEL", "tinyllama")
llm = OllamaLLM(model=MODELS[MODEL_NAME]["model"])
```

### Agregar Nuevo Modelo

1. Editar `app/models_config.py`:
```python
MODELS = {
    "nuevo_modelo": {
        "model": "nuevo_modelo:version",
        "name": "Nuevo Modelo",
        "size": "XB",
        "ram": "~X GB",
        "description": "DescripciÃ³n del modelo"
    }
}
```

2. Actualizar `change-model.sh` con el nuevo modelo

### Testing Local

```bash
# Desarrollo sin Docker
pip install -r requirements.txt
uvicorn app.main:app --reload

# Testing con modelo especÃ­fico
OLLAMA_MODEL=tinyllama python -m uvicorn app.main:app --reload
```

## ğŸ› SoluciÃ³n de Problemas

### Problemas Comunes

#### Error: "model not found"
```bash
# Verificar modelo disponible
docker exec -it proyecto-chat-rag-ollama-1 ollama list

# Descargar modelo manualmente
docker exec -it proyecto-chat-rag-ollama-1 ollama pull tinyllama
```

#### RAM insuficiente
```bash
# Usar modelo mÃ¡s ligero
./change-model.sh tinyllama

# O configurar directamente
OLLAMA_MODEL=tinyllama docker-compose up --build
```

#### Contenedor no inicia
```bash
# Ver logs
docker-compose logs app
docker-compose logs ollama

# Reiniciar limpio
docker-compose down -v
docker-compose up --build
```

### Logs y Debugging

```bash
# Ver logs en tiempo real
docker-compose logs -f app

# Acceder al contenedor
docker exec -it proyecto-chat-rag-app-1 bash

# Estado de Ollama
curl http://localhost:11434/api/tags
```

## ğŸ¤ Contribuir

1. Fork del proyecto
2. Crear rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m 'Agregar nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver [LICENSE](LICENSE) para detalles.

## ğŸ™ Reconocimientos

- [Ollama](https://ollama.ai/) - Modelos de lenguaje local
- [LangChain](https://langchain.com/) - Framework RAG
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [Chroma](https://www.trychroma.com/) - Base de datos vectorial

---

**Â¿Preguntas?** Abre un [Issue](../../issues) o consulta la [documentaciÃ³n de la API](http://localhost:8000/docs).cto Chat RAG Local

Servidor local RAG (Retrieval-Augmented Generation) con:
- FastAPI + Uvicorn
- LangChain + Ollama
- IngestiÃ³n automÃ¡tica de archivos (PDF, DOCX, TXT, CSV, MD, PPTX, HTML, JSON, imÃ¡genes y videos)
- Filtrado ZIP y subdirectorios

## Despliegue con Docker

1. Construir y levantar contenedores:

   ```bash
   docker-compose up --build
   ```

2. Abrir en el navegador:

   http://localhost:8000

## Estructura del Proyecto

```
proyecto-chat-rag/
â”œâ”€â”€ app/                 # CÃ³digo del servidor FastAPI
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/                # Documentos de usuario (subdirectorios permitidos)
â”œâ”€â”€ scripts/             # Scripts auxiliares (ingestiÃ³n de datos)
â”‚   â””â”€â”€ ingest.py
â”œâ”€â”€ vectorstore/         # Ãndice vectorial guardado
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

## Formatos soportados

- Texto: .txt, .md, .csv, .json, .html
- Office: .pdf, .docx, .pptx
- ImÃ¡genes: .png, .jpg, .jpeg, .tiff (OCR con Tesseract)
- Videos: .mp4, .avi (transcripciÃ³n con Whisper)

